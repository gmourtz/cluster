# Common variables for all hosts
---
ansible_python_interpreter: "/usr/bin/python3"
ansible_become: true
ansible_become_method: sudo
ansible_become_user: root

# Global cluster configuration
cluster_name: homelab
env: production

# Spark
spark_version: "3.5.1"
spark_dist: "without-hadoop"               # avoids x86 native libs
spark_pkg: "spark-{{ spark_version }}-bin-{{ spark_dist }}"
spark_tgz_url: "https://archive.apache.org/dist/spark/spark-{{ spark_version }}/{{ spark_pkg }}.tgz"

spark_root: /opt/spark
spark_install_dir: "{{ spark_root }}/{{ spark_pkg }}"
spark_link_dir: "{{ spark_root }}/current"
spark_user: spark
spark_group: spark

# Paths and dirs
spark_work_dir: /var/lib/spark/work
spark_local_dir: /var/lib/spark/local
spark_eventlog_dir: /var/lib/spark/eventlog
spark_logs_dir: /var/log/spark

# Networking
spark_master_port: 7077
spark_master_ui: 8080
spark_worker_ui: 8081
spark_history_ui: 18080
# Dynamic reference to master from inventory
spark_master_host: "{{ hostvars[groups['spark_masters'][0]]['ansible_host'] }}"
spark_master_url: "spark://{{ spark_master_host }}:{{ spark_master_port }}"


# Defaults tuned for small ARM workers
spark_defaults:
  spark.master: "{{ spark_master_url }}"
  spark.serializer: "org.apache.spark.serializer.KryoSerializer"
  # Event logging: enables Spark to write job history to disk for later analysis
  # This is what the History Server reads to reconstruct job details
  spark.eventLog.enabled: "true"
  spark.eventLog.dir: "file:{{ spark_eventlog_dir }}"
  # History Server configuration: where to read completed job logs from
  spark.history.fs.logDirectory: "file:{{ spark_eventlog_dir }}"
  spark.local.dir: "{{ spark_local_dir }}"
  spark.io.compression.codec: "lz4"
  spark.sql.parquet.compression.codec: "lz4"
  spark.sql.execution.arrow.pyspark.enabled: "false"
  spark.executor.cores: "1"
  spark.executor.memory: "512m"
  spark.memory.fraction: "0.4"
  spark.sql.autoBroadcastJoinThreshold: "16m"
  spark.shuffle.file.buffer: "16k"
  spark.reducer.maxSizeInFlight: "16m"
  spark.shuffle.spill: "true"

# Java
java_packages:
  - openjdk-11-jre-headless

# Ulimits
nofile_limit: 100000

# set a state flag
spark_state: present   # change to 'absent' to destroy
remove_java: false     # set true when destroying if you want Java gone too



